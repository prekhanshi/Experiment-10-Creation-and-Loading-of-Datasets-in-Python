# Experiment-10-Creation-and-Loading-of-Datasets-in-Python
Experiment-10

# Name: Prekhanshi Kumbhakar

# PRN: 250701230151

# Batch: ENTC A1

# Title

Creation and Loading of Datasets in Python

# Aim

To study the process of creating custom datasets from dictionaries, saving them as CSV files, and loading external datasets for data exploration using the Pandas library.

# Objectives

To create a custom student database using Python dictionaries.
To export a DataFrame into a CSV format for persistent storage.
To load external CSV datasets into a Pandas DataFrame.
To perform metadata exploration and basic statistical summaries.
To understand coordinate-based data retrieval using loc and iloc.
# Theory

Data analysis typically begins with either data creation or data ingestion. Python provides efficient tools to handle both stages of the workflow.

Dataset Creation and Exporting
Datasets can be manually constructed using Python Dictionaries, where keys represent column headers and values are lists of data points. Once converted into a DataFrame, the data can be exported using the to_csv() method, which converts the in-memory object into a permanent file format.

Loading External Datasets
The read_csv() function is the primary tool for data ingestion. It translates raw CSV text into a structured DataFrame object, allowing analysts to work with massive datasets (like the Cars93 dataset) without manual entry.

# Data Exploration Operations

info(): Provides a concise summary of the DataFrame, including the number of non-null entries and the data types of each column.
shape: Returns a tuple representing the dimensions (rows and columns) of the dataset.
head() / tail(): Displays the first or last five rows, respectively, to quickly inspect data integrity.
dtypes: Lists the data type of every column, such as int64 for whole numbers and float64 for decimals.
Data Retrieval

Label-based (loc): Used to access rows or specific cells by their index labels or column names.
Integer-based (iloc): Accesses data using numerical positions (coordinates), which is useful when the index labels are unknown.
Statistical Summaries

Central Tendency: Calculations for the Mean (average), Median (middle value), and Mode (most frequent value) are performed to describe numerical and categorical distributions.
Applications of Dataset Management

Database Migration: Moving local dictionary data into centralized storage systems.
Automated Reporting: Loading updated CSV files daily to generate automated business metrics.
Large Scale Analysis: Handling diverse datasets like automotive performance records for comparative research.
Data Auditing: Using info() and shape to check for missing values or incorrect data entries in uploaded files.
# Conclusion

The experiment successfully demonstrates the end-to-end process of data management in Python. By constructing a custom student_database.csv and loading the Cars93.csv file, we established the ability to both generate and ingest data. The application of statistical methods (mean, median, mode) and retrieval techniques (loc, iloc) ensures that data is not only stored but also actionable and insightful.
